{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/research_projects/rag-end2end-retriever/finetune_rag.py\n",
    "import numpy as np\n",
    "import psutil\n",
    "from sys import getsizeof\n",
    "import os\n",
    "from transformers import T5Tokenizer\n",
    "from icecream import ic\n",
    "import time\n",
    "from transformers import (DPRContextEncoder,\n",
    "                          DPRQuestionEncoder,\n",
    "                          DPRContextEncoderTokenizerFast,\n",
    "                          DPRQuestionEncoderTokenizerFast,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModel)\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "class CorpusDataset(pd.DataFrame):\n",
    "    def __init__(self, path_to_file: str):\n",
    "        super().__init__(self.get_df(path_to_file))\n",
    "\n",
    "    def get_df(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        return df\n",
    "\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, path_to_file: str, text_column: str, corpus, nb_irrelevant=1):\n",
    "        self.df = self.get_df(path_to_file, text_column)\n",
    "        self.corpus = corpus\n",
    "        self.nb_irrelevant = nb_irrelevant\n",
    "        self.count_doc = [0 for _ in range(len(self.df))]\n",
    "\n",
    "    def get_df(self, path, text_column=None):\n",
    "        def parse_ids(ids_str):\n",
    "            ids_list = [id[1:-1] for id in ids_str[1:-1].split(\", \")]\n",
    "            return ids_list\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        columns = [\"query\",\n",
    "                   \"outline\",\n",
    "                   \"text\",\n",
    "                   \"paragraphs_id\"]\n",
    "        df = df[[\"query\",\n",
    "                 \"outline\",\n",
    "                 \"text_\" + text_column,\n",
    "                 \"paragraphs_id\"]]\n",
    "        df = df.loc[df[\"outline\"].map(len) > 0]\n",
    "        df[\"text\"] = df[\"text_\" + text_column]\n",
    "        df[\"paragraphs_id\"] = df[\"paragraphs_id\"].apply(parse_ids)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        df = df[columns]\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        row = self.df.iloc[item]\n",
    "        docs_id = row[\"paragraphs_id\"]\n",
    "        query = {\"query\": row[\"query\"],\n",
    "                 \"text\": row[\"text\"]}\n",
    "        positive = self.get_positive(docs_id[self.count_doc[item]])\n",
    "        negative = self.get_negative(docs_id)\n",
    "        element = {\"query\": query,\n",
    "                   \"positive\": positive,\n",
    "                   \"negative\": negative}\n",
    "        self.count_doc[item] = (self.count_doc[item] + 1) % len(docs_id)\n",
    "        return element\n",
    "\n",
    "    def get_positive(self, doc_id):\n",
    "        positive = {\"doc_id\": doc_id}\n",
    "        row = self.corpus[self.corpus[\"id\"] == doc_id]\n",
    "        positive[\"text\"] = row[\"text\"].item()\n",
    "        # positive[\"input_ids\"] = row[\"input_ids\"].values[0]\n",
    "        # positive[\"attention_mask\"] = row[\"attention_mask\"].values[0]\n",
    "        return positive\n",
    "\n",
    "    def get_negative(self, docs_id_positive):\n",
    "        negatives_documents = []\n",
    "        ids_selected = []\n",
    "        for _ in range(self.nb_irrelevant):\n",
    "            item_random = np.random.randint(0, len(self.corpus))\n",
    "            row_negative = self.corpus.iloc[item_random]\n",
    "            while row_negative[\"id\"] in docs_id_positive + ids_selected:\n",
    "                item_random = np.random.randint(0, len(self.corpus))\n",
    "                row_negative = self.corpus.iloc[item_random]\n",
    "            ids_selected.append(row_negative[\"id\"])\n",
    "            negative = {\"doc_id\": row_negative[\"id\"],\n",
    "                        \"text\": row_negative[\"text\"]}\n",
    "            # \"input_ids\": row_negative[\"input_ids\"],\n",
    "            # \"attention_mask\": row_negative[\"attention_mask\"]}\n",
    "            negatives_documents.append(negative)\n",
    "        return negatives_documents\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class DPR(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of the DPR module :\n",
    "    Encode all documents (contexts), and query with different BERT encoders.\n",
    "    Similarity measure with dot product.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 query_model_name: str,\n",
    "                 context_model_name: str,\n",
    "                 train_val_test : tuple,\n",
    "                 batch_size=2,\n",
    "                 num_workers=5,\n",
    "                 learning_rate=1e-6):\n",
    "        super().__init__()\n",
    "        logging.info(\"\\n\\nWARNING about [query_tokenizer] :\")\n",
    "        # self.query_tokenizer = DPRQuestionEncoderTokenizerFast.from_pretrained(query_model_name)\n",
    "        self.query_tokenizer = AutoTokenizer.from_pretrained(query_model_name)\n",
    "        logging.info(\"\\nWARNING about [context_tokenizer]\")\n",
    "        # self.context_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(context_model_name)\n",
    "        self.context_tokenizer = AutoTokenizer.from_pretrained(context_model_name)\n",
    "        logging.info(\"\\nWARNING about [query_model]\")\n",
    "        # self.query_model = DPRQuestionEncoder.from_pretrained(query_model_name)\n",
    "        self.query_model = DPRQuestionEncoder.from_pretrained(query_model_name)\n",
    "        logging.info(\"\\nWARNING about [context_model]\")\n",
    "        # self.context_model = DPRContextEncoder.from_pretrained(context_model_name)\n",
    "        self.context_model = DPRContextEncoder.from_pretrained(context_model_name)\n",
    "        logging.info(\"\\n\\n\")\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_ds, self.val_ds, self.test_ds = train_val_test\n",
    "\n",
    "        self.loss_fn = self.get_loss_fn()\n",
    "\n",
    "        if num_workers == -1:\n",
    "            logging.info(f\"Number of CPUs available : {psutil.cpu_count()}.\")\n",
    "            self.num_workers = psutil.cpu_count()\n",
    "        else:\n",
    "            self.num_workers = num_workers\n",
    "\n",
    "    def encode_queries(self, query: dict):\n",
    "        query_encoding = self.query_tokenizer(query[\"text\"],\n",
    "                                              truncation=True,\n",
    "                                              max_length=512,\n",
    "                                              padding=\"max_length\",\n",
    "                                              return_tensors=\"pt\")\n",
    "        query[\"input_ids\"] = query_encoding[\"input_ids\"]\n",
    "        query[\"attention_mask\"] = query_encoding[\"attention_mask\"]\n",
    "        return query\n",
    "\n",
    "    def encode_contexts(self, contexts):\n",
    "        def process(context):\n",
    "            contexts_encoding = self.context_tokenizer(context[\"text\"],\n",
    "                                                       truncation=True,\n",
    "                                                       max_length=512,\n",
    "                                                       padding=\"max_length\",\n",
    "                                                       return_tensors='pt')\n",
    "            context[\"input_ids\"] = contexts_encoding[\"input_ids\"]\n",
    "            context[\"attention_mask\"] = contexts_encoding[\"attention_mask\"]\n",
    "            return context\n",
    "\n",
    "        if type(contexts) == list:\n",
    "            for i in range(len(contexts)):\n",
    "                contexts[i] = process(contexts[i])\n",
    "        elif type(contexts) == dict:\n",
    "            contexts = process(contexts)\n",
    "        return contexts\n",
    "\n",
    "    def decode_contexts(self, contexts_encodings: list):\n",
    "        contexts = [self.context_tokenizer.decode(c) for c in contexts_encodings]\n",
    "        return contexts\n",
    "\n",
    "    def get_dense_query(self, query):\n",
    "        query = self.encode_queries(query)\n",
    "        dense_query = self.query_model(input_ids=query[\"input_ids\"].to(self.device),\n",
    "                                       attention_mask=query[\"attention_mask\"].to(self.device))[\"pooler_output\"]\n",
    "        return dense_query\n",
    "\n",
    "    def get_dense_contexts(self, contexts):\n",
    "        contexts = self.encode_contexts(contexts)\n",
    "        dense_embeddings = []\n",
    "        for context in contexts:\n",
    "            embedding = self.context_model(input_ids=context[\"input_ids\"].to(self.device),\n",
    "                                           attention_mask=context[\"attention_mask\"].to(self.device))\n",
    "            dense_embeddings.append(embedding[\"pooler_output\"])\n",
    "        return torch.cat(dense_embeddings)\n",
    "\n",
    "    def dot_product(self, query, contexts):\n",
    "        sim = query.squeeze().matmul(contexts.T)\n",
    "        return sim.squeeze()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(copy.deepcopy(self.train_ds),\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(copy.deepcopy(self.val_ds),\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(copy.deepcopy(self.test_ds),\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def get_loss_fn(self, own=True):\n",
    "        \"\"\"negative log likelihood from DPR paper.\"\"\"\n",
    "        if own:\n",
    "            loss_fn = lambda similarity: -torch.log(similarity[0].exp() / similarity.exp().sum())\n",
    "            return loss_fn\n",
    "        else:\n",
    "            nllloss = nn.NLLLoss()\n",
    "            loss_fn = lambda prediction: nllloss(prediction, torch.tensor(0).to(self.device))\n",
    "            return loss_fn\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                     lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        :param query:\n",
    "        :param return_contexts:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.context_model.train()\n",
    "        self.query_model.train()\n",
    "        loss = 0\n",
    "        for data in batch:\n",
    "            query_dense = self.get_dense_query(data[\"query\"])\n",
    "            contexts_dense = self.get_dense_contexts([data[\"positive\"], *data[\"negative\"]])\n",
    "            similarity = self.dot_product(query_dense, contexts_dense)\n",
    "            loss += self.loss_fn(similarity)\n",
    "\n",
    "        self.log('train/loss_step', loss.item(), on_step=True, batch_size=self.batch_size)\n",
    "        self.log('train/loss_epoch', loss.item(), on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.train_loss = loss\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        logging.info(f'Finishing  epoch {str(self.current_epoch).rjust(5)} - loss : {str(self.train_loss).rjust(15)}')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.context_model.eval()\n",
    "        self.query_model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in batch:\n",
    "                query_dense = self.get_dense_query(data[\"query\"])\n",
    "                contexts_dense = self.get_dense_contexts([data[\"positive\"], *data[\"negative\"]])\n",
    "                similarity = self.dot_product(query_dense, contexts_dense)\n",
    "                loss += self.loss_fn(similarity)\n",
    "\n",
    "            self.log('Val/loss_step', loss.item(), on_step=True, batch_size=self.batch_size)\n",
    "            self.log('Val/loss_epoch', loss.item(), on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
    "            self.val_loss = loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        logging.info(f'Validation epoch {str(self.current_epoch).rjust(5)} - loss : {str(self.val_loss).rjust(15)}')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.context_model.eval()\n",
    "        self.query_model.eval()\n",
    "        for data in batch:\n",
    "            query_dense = self.get_dense_query(data[\"query\"])\n",
    "            contexts_dense = self.get_dense_contexts([data[\"positive\"], *data[\"negative\"]])\n",
    "            similarity = self.dot_product(query_dense, contexts_dense)\n",
    "            print(similarity)\n",
    "        return \"0\"+str(batch_idx),\"1\"+str(batch_idx),\"2\"+str(batch_idx)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.test_predictions = sum([output[0] for output in outputs], [])\n",
    "        self.test_actuals = sum([output[1] for output in outputs], [])\n",
    "        self.test_outlines = sum([output[2] for output in outputs], [])\n",
    "\n",
    "    def predict(self, trainer, corpus):\n",
    "        trainer.test(self)\n",
    "        self.contexts_encoded = self.encode_contexts()\n",
    "        return self.test_predictions, self.test_actuals, self.test_outlines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logging.info(\"make model : dpr()\")\n",
    "text_column = \"w/o_heading_first_sentence_by_paragraph\"\n",
    "\n",
    "logging.info(\" \" * 35 + \"↪ elapsed time : \"\n",
    "                        f\"{int((time.time() - start_time) // 60)}min \"\n",
    "                        f\"{(time.time() - start_time) % 60:.2f}s.\")\n",
    "logging.info(f\"Get corpus\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_train = CorpusDataset(\n",
    "    # path_to_file=\"/users/iris/rserrano/data-set_pre_processed/fold-1/corpus_train.csv\",\n",
    "    path_to_file=\"../../../data-subset_pre_processed/fold-1/corpus_train.csv\",\n",
    "    # context_encoder=dpr.encode_contexts\n",
    ")\n",
    "corpus_val = CorpusDataset(\n",
    "    # path_to_file=\"/users/iris/rserrano/data-set_pre_processed/fold-2/corpus_train.csv\",\n",
    "    path_to_file=\"../../../data-subset_pre_processed/fold-2/corpus_train.csv\",\n",
    "    # context_encoder=dpr.encode_contexts\n",
    ")\n",
    "corpus_test = CorpusDataset(\n",
    "    # path_to_file=\"/users/iris/rserrano/data-set_pre_processed/fold-3/corpus_train.csv\",\n",
    "    path_to_file=\"../../../data-subset_pre_processed/fold-3/corpus_train.csv\",\n",
    "    # context_encoder=dpr.encode_contexts\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_train = QueryDataset(\n",
    "    # path_to_file=\"/users/iris/rserrano/data-set_pre_processed/fold-1/articles_train_all_ids.csv\",\n",
    "    path_to_file=\"../../../data-subset_pre_processed/fold-1/articles_train_all_ids.csv\",\n",
    "    text_column=text_column,\n",
    "    # query_encoder=dpr.encode_queries,\n",
    "    corpus=corpus_train,\n",
    "    nb_irrelevant=2\n",
    ")\n",
    "ds_val = QueryDataset(\n",
    "    # path_to_file=\"/users/iris/rserrano/data-set_pre_processed/fold-2/articles_train_all_ids.csv\",\n",
    "    path_to_file=\"../../../data-subset_pre_processed/fold-2/articles_train_all_ids.csv\",\n",
    "    text_column=text_column,\n",
    "    # query_encoder=dpr.encode_queries,\n",
    "    corpus=corpus_val\n",
    ")\n",
    "ds_test = QueryDataset(\n",
    "    # path_to_file=\"/users/iris/rserrano/data-set_pre_processed/fold-3/articles_train_all_ids.csv\",\n",
    "    path_to_file=\"../../../data-subset_pre_processed/fold-3/articles_train_all_ids.csv\",\n",
    "    text_column=text_column,\n",
    "    # query_encoder=dpr.encode_queries,\n",
    "    corpus=corpus_test\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logger = pl_loggers.TensorBoardLogger(\"./dpr/checkpoints\", name=\"dpr_retriever\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"Val/loss_epoch\", mode=\"min\", save_top_k=2, every_n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(logger=logger,\n",
    "                  precision=32,\n",
    "                  accelerator=\"gpu\",\n",
    "                  gpus=-1,\n",
    "                  strategy='dp',\n",
    "                  max_epochs=100,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  log_every_n_steps=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dpr = DPR(context_model_name=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "          query_model_name=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "          train_val_test=(ds_train, ds_val, ds_test))\n",
    "trainer.test(model=dpr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}