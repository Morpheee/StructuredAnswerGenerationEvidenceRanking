{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/research_projects/rag-end2end-retriever/finetune_rag.py\n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "import os.path\n",
    "from transformers import T5Tokenizer\n",
    "from icecream import ic\n",
    "import time\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizerFast\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, path_to_file: str, text_column: str, query_encoder):\n",
    "        self.df = self.get_df(path_to_file, text_column, query_encoder)\n",
    "\n",
    "    def get_df(self, path, text_column=None, query_encoder=None):\n",
    "        def parse_ids(ids_str):\n",
    "            ids_list = [id[1:-1] for id in ids_str[1:-1].split(\", \")]\n",
    "            return ids_list\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        columns = [\"query\",\n",
    "                   \"outline\",\n",
    "                   \"text\",\n",
    "                   \"paragraph_span\",\n",
    "                   \"paragraph_id\"]\n",
    "        df = df[[\"query\",\n",
    "                 \"outline\",\n",
    "                 \"text_\" + text_column,\n",
    "                 \"paragraph_span_\" + text_column,\n",
    "                 \"paragraph_id_\" + text_column]]\n",
    "        df = df.loc[df[\"outline\"].map(len) > 0]\n",
    "        df[\"text\"] = df[\"text_\" + text_column]\n",
    "        df[\"paragraph_span\"] = df[\"paragraph_span_\" + text_column]\n",
    "        df[\"paragraph_id\"] = df[\"paragraph_id_\" + text_column].apply(parse_ids)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        df = df[columns]\n",
    "        df = query_encoder(df)\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        row = self.df.iloc[item]\n",
    "        query = row[\"query\"]\n",
    "        ids = row[\"paragraph_id\"]\n",
    "        input_ids = row[\"input_ids\"]\n",
    "        attention_mask = row[\"attention_mask\"]\n",
    "        return {\"query\": query, \"ids\": ids, \"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "class CorpusDataset(pd.DataFrame):\n",
    "    def __init__(self, path_to_file: str):\n",
    "        df = self.get_df(path_to_file)\n",
    "        super().__init__(df)\n",
    "\n",
    "    def get_df(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class DPR(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the DPR module :\n",
    "    Encode all documents (contexts), and query with different BERT encoders.\n",
    "    Similarity measure with dot product.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 query_model_name: str,\n",
    "                 context_model_name: str,\n",
    "                 contexts,\n",
    "                 dense_size=64):\n",
    "        super().__init__()\n",
    "        self.query_tokenizer = DPRQuestionEncoderTokenizerFast.from_pretrained(query_model_name)\n",
    "        self.context_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(context_model_name)\n",
    "        self.query_model = DPRQuestionEncoder.from_pretrained(query_model_name)\n",
    "        self.context_model = DPRContextEncoder.from_pretrained(context_model_name)\n",
    "        self.contexts = contexts\n",
    "        self.encode_contexts()\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.contexts_to_dense = nn.Sequential(nn.Linear(768, dense_size * 2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Linear(dense_size * 2, dense_size),\n",
    "                                               nn.GELU())\n",
    "        self.query_to_dense = nn.Sequential(nn.Linear(768, dense_size * 2),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(dense_size * 2, dense_size),\n",
    "                                            nn.GELU())\n",
    "\n",
    "    def encode_contexts(self, contexts: pd.DataFrame = None):\n",
    "        ctx_enc = lambda ctx: self.context_tokenizer(ctx, truncation=True, padding=True, return_tensors='pt')\n",
    "        if contexts is not None:\n",
    "            contexts_encoding = pd.DataFrame(contexts[\"text\"].apply(ctx_enc).tolist())\n",
    "            contexts[\"input_ids\"] = contexts_encoding[\"input_ids\"]\n",
    "            contexts[\"attention_mask\"] = contexts_encoding[\"attention_mask\"]\n",
    "            return contexts\n",
    "        else:\n",
    "            contexts_encoding = pd.DataFrame(self.contexts[\"text\"].apply(ctx_enc).tolist())\n",
    "            self.contexts[\"input_ids\"] = contexts_encoding[\"input_ids\"]\n",
    "            self.contexts[\"attention_mask\"] = contexts_encoding[\"attention_mask\"]\n",
    "            return self.contexts\n",
    "\n",
    "    def decode_contexts(self, contexts_encodings: list):\n",
    "        contexts = [self.context_tokenizer.decode(c) for c in contexts_encodings]\n",
    "        return contexts\n",
    "\n",
    "    def get_dense_contexts(self, contexts=None, return_tensor=False):\n",
    "        dense_emb = lambda ids, mask: self.context_model(input_ids=ids, attention_mask=mask)\n",
    "        dense = contexts[[\"input_ids\", \"attention_mask\"]].apply(lambda x: dense_emb(x[0], x[1]), axis=1)\n",
    "        dense = dense.apply(lambda x: self.contexts_to_dense(x[\"pooler_output\"]).squeeze())\n",
    "        return dense\n",
    "\n",
    "    def encode_queries(self, queries):\n",
    "        qry_enc = lambda qry: self.query_tokenizer(qry, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        queries_encoding = pd.DataFrame(queries[\"text\"].apply(qry_enc).tolist())\n",
    "        queries[\"input_ids\"] = queries_encoding[\"input_ids\"]\n",
    "        queries[\"attention_mask\"] = queries_encoding[\"attention_mask\"]\n",
    "        return queries\n",
    "\n",
    "    def get_dense_query(self, query):\n",
    "        dense_query = self.query_model(input_ids=query[\"input_ids\"],\n",
    "                                       attention_mask=query[\"attention_mask\"])\n",
    "        dense_query = self.query_to_dense(dense_query[\"pooler_output\"]).squeeze()\n",
    "        return dense_query\n",
    "\n",
    "    def dot_product(self, q_vector, p_vector):\n",
    "        q_vector = q_vector.unsqueeze(1)\n",
    "        sim = torch.matmul(q_vector, torch.transpose(p_vector, -2, -1))\n",
    "        return sim\n",
    "\n",
    "    def context_to_tensor(self, contextx_dense):\n",
    "        tensor = []\n",
    "        for row in contextx_dense:\n",
    "            tensor.append(row.detach().numpy())\n",
    "        return torch.tensor(tensor)\n",
    "\n",
    "    def forward_step(self, query_dense, contexts_dense_tensor, k=10):\n",
    "        \"\"\"\n",
    "        :param query:\n",
    "        :param return_contexts:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        similarity = (contexts_dense_tensor @ query_dense.unsqueeze(1)).squeeze()\n",
    "        top_k = similarity.argsort() < k\n",
    "        return top_k\n",
    "\n",
    "    def forward(self, query_batch):\n",
    "        self.contexts[\"dense\"] = self.get_dense_contexts(self.contexts)\n",
    "        contexts_dense_tensor = self.context_to_tensor(self.contexts[\"dense\"])\n",
    "        for query in query_batch:\n",
    "            query_dense = self.get_dense_query(query)\n",
    "            pred = self.forward_step(query_dense, contexts_dense_tensor, k=len(query[\"ids\"]))\n",
    "            ground_truth = torch.tensor(self.contexts[\"id\"].apply(lambda x: x in query[\"ids\"]).tolist())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "corpus_train = CorpusDataset(path_to_file=\"../../../data_pre_processed/fold-1/corpus_train.csv\")\n",
    "\n",
    "corpus_train = corpus_train.sample(60).reset_index(drop=True)\n",
    "# corpus_train = pd.read_csv(\"../../../data_pre_processed/fold-1/corpus_train.csv\").sample(25).reset_index(drop=True)\n",
    "\n",
    "# df_val = df_get(\"../../data_pre_processed/fold-2/articles_train.csv\",\n",
    "#                 text_column=text_column)\n",
    "# corpus_val = pd.read_csv(\"../../data_pre_processed/fold-2/corpus_train.csv\").sample(25)\n",
    "#\n",
    "# df_test = df_get(\"../../data_pre_processed/fold-3/articles_train.csv\",\n",
    "#                  text_column=text_column)\n",
    "# corpus_test = pd.read_csv(\"../../data_pre_processed/fold-3/corpus_train.csv\").sample(25)\n",
    "\n",
    "#---------------------------------------------------------------)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dpr = DPR(context_model_name=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "          query_model_name=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "          contexts=corpus_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "text_column = \"w/o_heading_first_sentence_by_paragraph\"\n",
    "\n",
    "df_train = QueryDataset(path_to_file=\"../../../data_pre_processed/fold-1/articles_train.csv\",\n",
    "                        text_column=text_column,\n",
    "                        query_encoder=dpr.encode_queries)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(df_train, batch_size=2, shuffle=True, collate_fn=lambda x: x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6500)\n",
      "tensor(0.7000)\n"
     ]
    }
   ],
   "source": [
    "for query_batch in train_dataset:\n",
    "    dpr.forward(query_batch)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legmint/.local/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unwrapping the module did not yield a `LightningModule`, got <class '__main__.DPR'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 721\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:800\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    799\u001B[0m \u001B[38;5;66;03m# links data to the trainer\u001B[39;00m\n\u001B[0;32m--> 800\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattach_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatamodule\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# TODO: ckpt_path only in v2.0\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:160\u001B[0m, in \u001B[0;36mDataConnector.attach_data\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, test_dataloaders, predict_dataloaders, datamodule)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m# set local properties on the model\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_copy_trainer_model_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:163\u001B[0m, in \u001B[0;36mDataConnector._copy_trainer_model_properties\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_copy_trainer_model_properties\u001B[39m(\u001B[38;5;28mself\u001B[39m, model):\n\u001B[0;32m--> 163\u001B[0m     ref_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m model\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m [model, ref_model]:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2134\u001B[0m, in \u001B[0;36mTrainer.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2131\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2133\u001B[0m     \u001B[38;5;66;03m# TODO: this is actually an optional return\u001B[39;00m\n\u001B[0;32m-> 2134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:312\u001B[0m, in \u001B[0;36mStrategy.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 312\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munwrap_lightning_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py:116\u001B[0m, in \u001B[0;36munwrap_lightning_module\u001B[0;34m(wrapped_model)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnwrapping the module did not yield a `LightningModule`, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[0;31mTypeError\u001B[0m: Unwrapping the module did not yield a `LightningModule`, got <class '__main__.DPR'> instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdpr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    751\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 768\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:735\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m distributed_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworld_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    733\u001B[0m     \u001B[38;5;66;03m# try syncing remaining processes, kill otherwise\u001B[39;00m\n\u001B[1;32m    734\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mreconciliate_processes(traceback\u001B[38;5;241m.\u001B[39mformat_exc())\n\u001B[0;32m--> 735\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_callback_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_exception\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexception\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n\u001B[1;32m    737\u001B[0m \u001B[38;5;66;03m# teardown might access the stage so we reset it after\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1617\u001B[0m, in \u001B[0;36mTrainer._call_callback_hooks\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1614\u001B[0m             fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m-> 1617\u001B[0m pl_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n\u001B[1;32m   1618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pl_module:\n\u001B[1;32m   1619\u001B[0m     prev_fx_name \u001B[38;5;241m=\u001B[39m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2134\u001B[0m, in \u001B[0;36mTrainer.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2131\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2133\u001B[0m     \u001B[38;5;66;03m# TODO: this is actually an optional return\u001B[39;00m\n\u001B[0;32m-> 2134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:312\u001B[0m, in \u001B[0;36mStrategy.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munwrap_lightning_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py:116\u001B[0m, in \u001B[0;36munwrap_lightning_module\u001B[0;34m(wrapped_model)\u001B[0m\n\u001B[1;32m    114\u001B[0m     model \u001B[38;5;241m=\u001B[39m unwrap_lightning_module(model\u001B[38;5;241m.\u001B[39mmodule)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnwrapping the module did not yield a `LightningModule`, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[0;31mTypeError\u001B[0m: Unwrapping the module did not yield a `LightningModule`, got <class '__main__.DPR'> instead."
     ]
    }
   ],
   "source": [
    "trainer.fit(model=dpr,train_dataloaders=train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}