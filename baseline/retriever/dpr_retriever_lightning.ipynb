{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/research_projects/rag-end2end-retriever/finetune_rag.py\n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "import os.path\n",
    "from transformers import T5Tokenizer\n",
    "from icecream import ic\n",
    "import time\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizerFast\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DPR(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of the DPR module :\n",
    "    Encode all documents (contexts), and query with different BERT encoders.\n",
    "    Similarity measure with dot product.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 query_model_name: str,\n",
    "                 context_model_name: str,\n",
    "                 train_batch_size=2,\n",
    "                 val_batch_size=2,\n",
    "                 learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        self.query_tokenizer = DPRQuestionEncoderTokenizerFast.from_pretrained(query_model_name)\n",
    "        self.context_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(context_model_name)\n",
    "        self.query_model = DPRQuestionEncoder.from_pretrained(query_model_name)\n",
    "        self.context_model = DPRContextEncoder.from_pretrained(context_model_name)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "\n",
    "        self.train_ds, self.val_ds, self.test_ds = None, None, None\n",
    "\n",
    "        # self.loss_fn = nn.NLLLoss()\n",
    "        self.loss_fn = self.get_loss_fn()\n",
    "\n",
    "    def get_sets(self, train_val_test):\n",
    "        self.train_ds, self.val_ds, self.test_ds = train_val_test\n",
    "\n",
    "    def encode_queries(self, queries):\n",
    "        qry_enc = lambda qry: self.query_tokenizer(qry, truncation=True,\n",
    "                                                   max_length=512,\n",
    "                                                   padding=\"max_length\",\n",
    "                                                   return_tensors=\"pt\")\n",
    "\n",
    "        queries_encoding = pd.DataFrame(queries[\"text\"].apply(qry_enc).tolist())\n",
    "        queries[\"input_ids\"] = queries_encoding[\"input_ids\"]\n",
    "        queries[\"attention_mask\"] = queries_encoding[\"attention_mask\"]\n",
    "        return queries\n",
    "\n",
    "    def encode_contexts(self, contexts: pd.DataFrame):\n",
    "        ctx_enc = lambda ctx: self.context_tokenizer(ctx, truncation=True,\n",
    "                                                     max_length=512,\n",
    "                                                     padding=\"max_length\",\n",
    "                                                     return_tensors='pt')\n",
    "        contexts_encoding = pd.DataFrame(contexts[\"text\"].apply(ctx_enc).tolist())\n",
    "        contexts[\"input_ids\"] = contexts_encoding[\"input_ids\"]\n",
    "        contexts[\"attention_mask\"] = contexts_encoding[\"attention_mask\"]\n",
    "        return contexts\n",
    "\n",
    "    def decode_contexts(self, contexts_encodings: list):\n",
    "        contexts = [self.context_tokenizer.decode(c) for c in contexts_encodings]\n",
    "        return contexts\n",
    "\n",
    "    def get_dense_query(self, query):\n",
    "        dense_query = self.query_model(input_ids=query[\"input_ids\"],\n",
    "                                       attention_mask=query[\"attention_mask\"])[\"pooler_output\"]\n",
    "        return dense_query\n",
    "\n",
    "    def get_dense_contexts(self, contexts):\n",
    "        dense_embeddings = []\n",
    "        for context in contexts:\n",
    "            embedding = self.context_model(input_ids=context[\"input_ids\"],\n",
    "                                           attention_mask=context[\"attention_mask\"])\n",
    "            dense_embeddings.append(embedding[\"pooler_output\"])\n",
    "        return torch.cat(dense_embeddings)\n",
    "\n",
    "    def dot_product(self, query, contexts):\n",
    "        sim = query.squeeze().matmul(contexts.T)\n",
    "        return sim.squeeze()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(copy.deepcopy(self.train_ds),\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(copy.deepcopy(self.val_ds),\n",
    "                          batch_size=self.val_batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(copy.deepcopy(self.test_ds),\n",
    "                          batch_size=self.val_batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def get_loss_fn(self):\n",
    "        \"\"\"negative log likelihood from DPR paper.\"\"\"\n",
    "\n",
    "        def loss_fn(similarity):\n",
    "            return -torch.log(similarity[0].exp() / similarity.exp().sum())\n",
    "\n",
    "        return loss_fn\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                     lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        :param query:\n",
    "        :param return_contexts:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.context_model.train()\n",
    "        self.query_model.train()\n",
    "        loss = 0\n",
    "        for data in batch :\n",
    "            query_dense = self.get_dense_query(data[\"query\"])\n",
    "            contexts_dense = self.get_dense_contexts([data[\"positive\"], *data[\"negative\"]])\n",
    "            similarity = self.dot_product(query_dense, contexts_dense)\n",
    "            loss += self.loss_fn(similarity)\n",
    "\n",
    "        loss /= len(batch)\n",
    "        self.log('train/loss_step', loss.item(), on_step=True)\n",
    "        self.log('train/loss_epoch', loss.item(), on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        print('Finishing epoch ', self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.context_model.eval()\n",
    "        self.query_model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in batch:\n",
    "                self.context_model.train()\n",
    "                self.query_model.train()\n",
    "                query_dense = self.get_dense_query(data[\"query\"])\n",
    "                contexts_dense = self.get_dense_contexts([data[\"positive\"], *data[\"negative\"]])\n",
    "                similarity = self.dot_product(query_dense, contexts_dense)\n",
    "                loss += self.loss_fn(similarity)\n",
    "\n",
    "            loss /= len(batch)\n",
    "            self.log('Val/loss_step', loss.item(), on_step=True)\n",
    "            self.log('Val/loss_epoch', loss.item(), on_step=False, on_epoch=True)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print('Validation epoch ', self.current_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dpr = DPR(context_model_name=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "          query_model_name=\"facebook/dpr-question_encoder-single-nq-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CorpusDataset(pd.DataFrame):\n",
    "    def __init__(self, path_to_file: str, context_encoder):\n",
    "        super().__init__(self.get_df(path_to_file, context_encoder))\n",
    "\n",
    "    def get_df(self, path, context_encoder):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        df = context_encoder(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, path_to_file: str, text_column: str, query_encoder, corpus, nb_irrelevant=1):\n",
    "        self.df = self.get_df(path_to_file, text_column, query_encoder)\n",
    "        self.corpus = corpus\n",
    "        self.nb_irrelevant = nb_irrelevant\n",
    "        self.count_doc = [0 for _ in range(len(self.df))]\n",
    "\n",
    "    def get_df(self, path, text_column=None, query_encoder=None):\n",
    "        def parse_ids(ids_str):\n",
    "            ids_list = [id[1:-1] for id in ids_str[1:-1].split(\", \")]\n",
    "            return ids_list\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        columns = [\"query\",\n",
    "                   \"outline\",\n",
    "                   \"text\",\n",
    "                   \"paragraphs_id\"]\n",
    "        df = df[[\"query\",\n",
    "                 \"outline\",\n",
    "                 \"text_\" + text_column,\n",
    "                 \"paragraphs_id\"]]\n",
    "        df = df.loc[df[\"outline\"].map(len) > 0]\n",
    "        df[\"text\"] = df[\"text_\" + text_column]\n",
    "        df[\"paragraphs_id\"] = df[\"paragraphs_id\"].apply(parse_ids)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        df = df[columns]\n",
    "        df = query_encoder(df)\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        row = self.df.iloc[item]\n",
    "        docs_id = row[\"paragraphs_id\"]\n",
    "        query = {\"query\": row[\"query\"],\n",
    "                 \"input_ids\": row[\"input_ids\"],\n",
    "                 \"attention_mask\": row[\"attention_mask\"]}\n",
    "        positive = self.get_positive(docs_id[self.count_doc[item]])\n",
    "        negative = self.get_negative(docs_id)\n",
    "        element = {\"query\": query,\n",
    "                   \"positive\": positive,\n",
    "                   \"negative\": negative}\n",
    "        self.count_doc[item] = (self.count_doc[item] + 1) % len(docs_id)\n",
    "        return element\n",
    "\n",
    "    def get_positive(self, doc_id):\n",
    "        positive = {\"doc_id\": doc_id}\n",
    "        row = self.corpus[self.corpus[\"id\"] == doc_id]\n",
    "        positive[\"input_ids\"] = row[\"input_ids\"].values[0]\n",
    "        positive[\"attention_mask\"] = row[\"attention_mask\"].values[0]\n",
    "        return positive\n",
    "\n",
    "    def get_negative(self, docs_id_positive):\n",
    "        negatives_documents = []\n",
    "        for _ in range(self.nb_irrelevant):\n",
    "            item_random = np.random.randint(0, len(self.corpus))\n",
    "            row_negative = self.corpus.iloc[item_random]\n",
    "            while row_negative[\"id\"] in docs_id_positive:\n",
    "                item_random = np.random.randint(0, len(self.corpus))\n",
    "                row_negative = self.corpus.iloc[item_random]\n",
    "            negative = {\"doc_id\": row_negative[\"id\"],\n",
    "                        \"input_ids\": row_negative[\"input_ids\"],\n",
    "                        \"attention_mask\": row_negative[\"attention_mask\"]}\n",
    "            negatives_documents.append(negative)\n",
    "        return negatives_documents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "corpus_train = CorpusDataset(path_to_file=\"../../../data-subset_pre_processed/fold-1/corpus_train.csv\",\n",
    "                             context_encoder=dpr.encode_contexts)\n",
    "corpus_val = CorpusDataset(path_to_file=\"../../../data-subset_pre_processed/fold-2/corpus_train.csv\",\n",
    "                           context_encoder=dpr.encode_contexts)\n",
    "corpus_test = CorpusDataset(path_to_file=\"../../../data-subset_pre_processed/fold-3/corpus_train.csv\",\n",
    "                            context_encoder=dpr.encode_contexts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "text_column = \"w/o_heading_first_sentence_by_paragraph\"\n",
    "\n",
    "ds_train = QueryDataset(path_to_file=\"../../../data-subset_pre_processed/fold-1/articles_train_all_ids.csv\",\n",
    "                        text_column=text_column,\n",
    "                        query_encoder=dpr.encode_queries,\n",
    "                        corpus=corpus_train,\n",
    "                        nb_irrelevant=2)\n",
    "# train_dataloader = DataLoader(ds_train, batch_size=2, shuffle=True)\n",
    "\n",
    "ds_val = QueryDataset(path_to_file=\"../../../data-subset_pre_processed/fold-2/articles_train_all_ids.csv\",\n",
    "                      text_column=text_column,\n",
    "                      query_encoder=dpr.encode_queries,\n",
    "                      corpus=corpus_val)\n",
    "# val_dataloader = DataLoader(ds_val, batch_size=2, shuffle=True)\n",
    "\n",
    "ds_test = QueryDataset(path_to_file=\"../../../data-subset_pre_processed/fold-3/articles_train_all_ids.csv\",\n",
    "                       text_column=text_column,\n",
    "                       query_encoder=dpr.encode_queries,\n",
    "                       corpus=corpus_test)\n",
    "# test_dataloader = DataLoader(ds_test, batch_size=2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dpr.get_sets(train_val_test=(ds_train, ds_val, ds_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "logger = pl_loggers.TensorBoardLogger(\"./checkpoints\", name=\"dpr_retriever\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"Val/loss_epoch\", mode=\"min\", save_top_k=2, every_n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(logger=logger,\n",
    "                  precision=32,\n",
    "                  gpus=-1,\n",
    "                  strategy='dp',\n",
    "                  max_epochs=100,\n",
    "                  callbacks=[checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ./checkpoints/dpr_retriever\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | query_model   | DPRQuestionEncoder | 108 M \n",
      "1 | context_model | DPRContextEncoder  | 108 M \n",
      "-----------------------------------------------------\n",
      "217 M     Trainable params\n",
      "0         Non-trainable params\n",
      "217 M     Total params\n",
      "871.133   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8102c001424f4a47b73728f76fff0934"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legmint/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legmint/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legmint/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9b23af8a32b4d3285f5ba28f89e2e91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 5.79 GiB total capacity; 4.46 GiB already allocated; 13.25 MiB free; 4.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdpr\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    751\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 768\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 721\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n\u001B[1;32m    723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:809\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    805\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    806\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[1;32m    807\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    808\u001B[0m )\n\u001B[0;32m--> 809\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1234\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1236\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1321\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1350\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1351\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:269\u001B[0m, in \u001B[0;36mFitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(\n\u001B[1;32m    266\u001B[0m     dataloader, batch_to_device\u001B[38;5;241m=\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_strategy_hook, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_to_device\u001B[39m\u001B[38;5;124m\"\u001B[39m, dataloader_idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    267\u001B[0m )\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:208\u001B[0m, in \u001B[0;36mTrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 208\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    212\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001B[0m, in \u001B[0;36mTrainingBatchLoop.advance\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m     87\u001B[0m     optimizers \u001B[38;5;241m=\u001B[39m _get_active_optimizers(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizer_frequencies, batch_idx)\n\u001B[0;32m---> 88\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     90\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_loop\u001B[38;5;241m.\u001B[39mrun(split_batch, batch_idx)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203\u001B[0m, in \u001B[0;36mOptimizerLoop.advance\u001B[0;34m(self, batch, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madvance\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: Any, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[0;32m--> 203\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_optimization\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim_progress\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_position\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001B[39;00m\n\u001B[1;32m    211\u001B[0m         \u001B[38;5;66;03m# would be skipped otherwise\u001B[39;00m\n\u001B[1;32m    212\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx] \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39masdict()\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256\u001B[0m, in \u001B[0;36mOptimizerLoop._run_optimization\u001B[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001B[0m\n\u001B[1;32m    249\u001B[0m         closure()\n\u001B[1;32m    251\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    258\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;66;03m# if no result, user decided to skip optimization\u001B[39;00m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001B[39;00m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;66;03m# TODO: find proper way to handle updating running loss\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:369\u001B[0m, in \u001B[0;36mOptimizerLoop._optimizer_step\u001B[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[1;32m    368\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 369\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_tpu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTPUAccelerator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_native_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mamp_backend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mAMPType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNATIVE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_lbfgs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_lbfgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1593\u001B[0m, in \u001B[0;36mTrainer._call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1590\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m   1592\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1593\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1595\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1596\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1625\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001B[0m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[1;32m   1544\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1545\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1552\u001B[0m     using_lbfgs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1553\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1554\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1555\u001B[0m \u001B[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;124;03m    each optimizer.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1623\u001B[0m \n\u001B[1;32m   1624\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1625\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 168\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:193\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;124;03m\"\"\"Performs the actual optimizer step.\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \n\u001B[1;32m    185\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    192\u001B[0m model \u001B[38;5;241m=\u001B[39m model \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\n\u001B[0;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:155\u001B[0m, in \u001B[0;36mPrecisionPlugin.optimizer_step\u001B[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[1;32m    154\u001B[0m     closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:100\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 100\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    103\u001B[0m     params_with_grad \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:140\u001B[0m, in \u001B[0;36mPrecisionPlugin._wrap_closure\u001B[0;34m(self, model, optimizer, optimizer_idx, closure)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    129\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    132\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m    133\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \n\u001B[1;32m    137\u001B[0m \u001B[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001B[39;00m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 140\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer, optimizer_idx)\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 148\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 134\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:427\u001B[0m, in \u001B[0;36mOptimizerLoop._training_step\u001B[0;34m(self, split_batch, batch_idx, opt_idx)\u001B[0m\n\u001B[1;32m    422\u001B[0m step_kwargs \u001B[38;5;241m=\u001B[39m _build_training_step_kwargs(\n\u001B[1;32m    423\u001B[0m     lightning_module, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, split_batch, batch_idx, opt_idx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hiddens\n\u001B[1;32m    424\u001B[0m )\n\u001B[1;32m    426\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[0;32m--> 427\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()\n\u001B[1;32m    430\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, training_step_output)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1762\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1763\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/dp.py:125\u001B[0m, in \u001B[0;36mDataParallelStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m STEP_OUTPUT:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mtrain_step_context():\n\u001B[0;32m--> 125\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:166\u001B[0m, in \u001B[0;36mDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m ({},)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m replicas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplicate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids[:\u001B[38;5;28mlen\u001B[39m(inputs)])\n\u001B[1;32m    168\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py:64\u001B[0m, in \u001B[0;36mLightningParallelModule.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_replica_device_attributes(inputs)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# forward call will redirect to training_step, validation_step, etc.\u001B[39;00m\n\u001B[0;32m---> 64\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moutput_transform\u001B[39m(data: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     67\u001B[0m     device \u001B[38;5;241m=\u001B[39m cast(torch\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py:82\u001B[0m, in \u001B[0;36m_LightningModuleWrapperBase.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m---> 82\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;66;03m# In manual_optimization, we need to prevent DDP reducer as\u001B[39;00m\n\u001B[1;32m     84\u001B[0m         \u001B[38;5;66;03m# it is done manually in `LightningModule.manual_backward`\u001B[39;00m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;66;03m# `require_backward_grad_sync` will be reset in the\u001B[39;00m\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;66;03m# ddp_strategy `post_training_step` hook\u001B[39;00m\n\u001B[1;32m     87\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m pl_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mDPR.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m batch :\n\u001B[1;32m    119\u001B[0m     query_dense \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_dense_query(data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m--> 120\u001B[0m     contexts_dense \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dense_contexts\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpositive\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnegative\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m     similarity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdot_product(query_dense, contexts_dense)\n\u001B[1;32m    122\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(similarity)\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mDPR.get_dense_contexts\u001B[0;34m(self, contexts)\u001B[0m\n\u001B[1;32m     63\u001B[0m dense_embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m context \u001B[38;5;129;01min\u001B[39;00m contexts:\n\u001B[0;32m---> 65\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     dense_embeddings\u001B[38;5;241m.\u001B[39mappend(embedding[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpooler_output\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(dense_embeddings)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/dpr/modeling_dpr.py:505\u001B[0m, in \u001B[0;36mDPRContextEncoder.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token_type_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    503\u001B[0m     token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m--> 505\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx_encoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/dpr/modeling_dpr.py:196\u001B[0m, in \u001B[0;36mDPREncoder.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    188\u001B[0m     input_ids: Tensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    194\u001B[0m     return_dict: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    195\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[BaseModelOutputWithPooling, Tuple[Tensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]:\n\u001B[0;32m--> 196\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    206\u001B[0m     pooled_output \u001B[38;5;241m=\u001B[39m sequence_output[:, \u001B[38;5;241m0\u001B[39m, :]\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:996\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    987\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    989\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    990\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    991\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    994\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    995\u001B[0m )\n\u001B[0;32m--> 996\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    997\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    998\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    999\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1000\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1001\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1002\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1006\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1008\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1009\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:585\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    576\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    577\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    578\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    582\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    583\u001B[0m     )\n\u001B[1;32m    584\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 585\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    587\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    595\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:472\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    462\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    470\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    471\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 472\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    479\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:402\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    394\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    400\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    401\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 402\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    411\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    412\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:324\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    321\u001B[0m         relative_position_scores_key \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbhrd,lrd->bhlr\u001B[39m\u001B[38;5;124m\"\u001B[39m, key_layer, positional_embedding)\n\u001B[1;32m    322\u001B[0m         attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m relative_position_scores_query \u001B[38;5;241m+\u001B[39m relative_position_scores_key\n\u001B[0;32m--> 324\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mattention_scores\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_head_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;66;03m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001B[39;00m\n\u001B[1;32m    327\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m attention_mask\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 5.79 GiB total capacity; 4.46 GiB already allocated; 13.25 MiB free; 4.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=dpr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# offline\n",
    "contexts_ids = DPRContextEncoderTokenizerFast(contexts)  # ids of vocabulary in Encoder's dictionary\n",
    "queries_ids = DPRQuestionEncoderTokenizerFast(queries)  # ids of vocabulary in Encoder's dictionary\n",
    "\n",
    "corpus = {\n",
    "    \"context_idex_1\": context_ids_1,\n",
    "    \"context_idex_2\": context_ids_2,\n",
    "    \"context_idex_3\": context_ids_3,\n",
    "    \"context_idex_4\": context_ids_4,\n",
    "    ...\n",
    "}\n",
    "\n",
    "batches = [\n",
    "    {\"queries_ids\": queries_ids,\n",
    "     \"contexts_indexes\": contexts_idexes},  # ground-truth of the documents indexes in corpus\n",
    "    {...},\n",
    "    ...\n",
    "]\n",
    "\n",
    "# online\n",
    "for epoch in epochs:\n",
    "    contexts_embedding = DPRContextEncoder(contexts_ids)\n",
    "\n",
    "    for batch in batches:\n",
    "        queries_embedding = DPRQuestionEncoder(batch[\"queries_ids\"])\n",
    "        top_k = get_to_k(contexts_embedding, queries_embedding)\n",
    "        loss = CrossEntropyLoss(top_k, batch[\"contexts_indexes\"])\n",
    "        loss.backward()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}