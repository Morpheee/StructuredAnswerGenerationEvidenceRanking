{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import (BartTokenizer,\n",
    "                          BartForConditionalGeneration,\n",
    "                          AdamW,\n",
    "                          BartConfig)\n",
    "from torch.utils.data import (DataLoader,\n",
    "                              TensorDataset,\n",
    "                              random_split,\n",
    "                              Dataset)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "import operator\n",
    "import copy\n",
    "import logging\n",
    "import psutil\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from data_utils import train_val_test_split_df\n",
    "from file_utils import mkdir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legmint/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# generated = model.generate(tokenizer(\"Hello world\",\n",
    "#                                      max_length=32,\n",
    "#                                      return_tensors=\"pt\",\n",
    "#                                      truncation=True)[\"input_ids\"],\n",
    "#                            attention_mask=tokenizer(\"Hello world\",\n",
    "#                                                     max_length=32,\n",
    "#                                                     return_tensors=\"pt\",\n",
    "#                                                     truncation=True)[\"attention_mask\"],\n",
    "#                            use_cache=True,\n",
    "#                            num_beams=4,\n",
    "#                            max_length=32,\n",
    "#                            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "#                            early_stopping=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "class CorpusDataset(pd.DataFrame):\n",
    "    def __init__(self, path_to_file: str, text_column: str = None):\n",
    "        super().__init__(self.get_df(path_to_file, text_column))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_df(path, text_column):\n",
    "        if path.endswith(\".json\"):\n",
    "            df = pd.read_json(path, dtype={\"id\" : str})\n",
    "        elif path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path)\n",
    "        if \"all\" in text_column:\n",
    "            df.rename(columns={\"all_passage\": \"text\"}, inplace=True)\n",
    "        elif \"first\" in text_column:\n",
    "            df.rename(columns={\"first_sentence\": \"text\"}, inplace=True)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        df[\"id\"] = df[\"id\"].apply(int)\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create a dataloading module as per the PyTorch Lightning Docs\n",
    "class AnswerGenerationData(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_to_df: str = None,\n",
    "                 df: pd.DataFrame = None,\n",
    "                 text_column: str = None,\n",
    "                 corpus: CorpusDataset = None):\n",
    "\n",
    "        assert operator.xor(path_to_df is None, df is None)\n",
    "        self.df = self.get_dataset(path_to_df=path_to_df,\n",
    "                                   df=df,\n",
    "                                   text_column=text_column,\n",
    "                                   corpus=corpus)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dataset(path_to_df: str = None,\n",
    "                    df: pd.DataFrame = None,\n",
    "                    text_column: str = None,\n",
    "                    corpus: CorpusDataset = None):\n",
    "\n",
    "        def get_df(path=None, df=None, text_column=None):\n",
    "            def parse_ids(ids):\n",
    "                # ids_list = [ids[1:-1] for ids in ids_str[1:-1].split(\", \")]\n",
    "                ids = [int(i) for i in ids]\n",
    "                return ids\n",
    "\n",
    "            if path is not None:\n",
    "                # df = pd.read_csv(path)\n",
    "                df = pd.read_json(path)\n",
    "            columns = [\"query\",\n",
    "                       \"outline\",\n",
    "                       \"text\",\n",
    "                       \"id\"]\n",
    "            df = df[[\"query\",\n",
    "                     \"outline\",\n",
    "                     \"text_\" + text_column,\n",
    "                     \"id\"]]\n",
    "            df = df.loc[df[\"outline\"].map(len) > 0]\n",
    "            df[\"text\"] = df[\"text_\" + text_column]\n",
    "            df[\"id\"] = df[\"id\"].apply(parse_ids)\n",
    "            df = df[columns]\n",
    "            df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "            return df\n",
    "\n",
    "        def get_paragraphs(query, docs_id, corpus):\n",
    "            stack = query\n",
    "            for index in docs_id:\n",
    "                stack += corpus[corpus[\"id\"] == index][\"text\"].item() + \"\\n\"\n",
    "            stack = stack[:-1]\n",
    "            return stack\n",
    "\n",
    "        assert operator.xor(path_to_df is not None, df is not None)\n",
    "\n",
    "        df = get_df(path_to_df, df, text_column)\n",
    "        corpus = corpus\n",
    "\n",
    "        df.rename(columns={\"text\": \"target\"}, inplace=True)\n",
    "        df[\"source\"] = df[[\"query\",\"id\"]].apply(lambda row: get_paragraphs(row[0], row[1], corpus), axis=1)\n",
    "        df = df[[\"source\", \"target\"]]  #, \"outline\"]]\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if type(item) == int:\n",
    "            return self.df[item:item + 1].to_dict('records')[0]\n",
    "        elif type(item) == str:\n",
    "            return self.df[item]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_datasets(path_to_file, path_to_corpus, text_column):\n",
    "    corpus = CorpusDataset(path_to_file=path_to_corpus, text_column=text_column)\n",
    "    df = pd.read_json(path_to_file)\n",
    "    df_train, df_val, df_test = train_val_test_split_df(df)\n",
    "    del df\n",
    "\n",
    "    df_train = AnswerGenerationData(df=df_train,\n",
    "                                    text_column=text_column,\n",
    "                                    corpus=corpus)\n",
    "    df_val = AnswerGenerationData(df=df_val,\n",
    "                                  text_column=text_column,\n",
    "                                  corpus=corpus)\n",
    "    df_test = AnswerGenerationData(df=df_test,\n",
    "                                   text_column=text_column,\n",
    "                                   corpus=corpus)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Bart(pl.LightningModule):\n",
    "    # Instantiate the model\n",
    "    def __init__(self,\n",
    "                 model_name='facebook/bart-base',\n",
    "                 train_val_test: tuple = None,\n",
    "                 freeze_params=-1,\n",
    "                 eval_beams=4,\n",
    "                 batch_size=32,\n",
    "                 num_workers=20,\n",
    "                 learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eval_beams = eval_beams\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if train_val_test is not None :\n",
    "            self.train_ds, self.val_ds, self.test_ds = train_val_test\n",
    "        else :\n",
    "            self.train_ds = None\n",
    "            self.val_ds = None\n",
    "            self.test_ds = None\n",
    "\n",
    "        self.freeze_params = freeze_params\n",
    "\n",
    "        if num_workers == -1:\n",
    "            logging.info(f\"Number of CPUs available : {psutil.cpu_count()}.\")\n",
    "            self.num_workers = psutil.cpu_count()\n",
    "        else:\n",
    "            self.num_workers = min(num_workers, psutil.cpu_count())\n",
    "\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "    def freeze_layers(self):\n",
    "        if self.freeze_params == -1:\n",
    "            self.freeze_last_layers()\n",
    "        else :\n",
    "            num_layers = sum(1 for _ in self.model.parameters())\n",
    "            for parameters in list(self.model.parameters())[:int(self.freeze_params * num_layers)]:\n",
    "                parameters.requires_grad = False\n",
    "        self.freeze_embeds()\n",
    "\n",
    "    def freeze_last_layers(self):\n",
    "        nb_layer = []\n",
    "        for name, _ in self.model.named_parameters():\n",
    "            try:\n",
    "                nb_layer.append(int(re.findall(r\"layer\\.\\d+\", name)[0].split(\".\")[-1]))\n",
    "            except IndexError:\n",
    "                continue\n",
    "        nb_layer = max(set(nb_layer))\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if f'layer.{nb_layer}' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def freeze_embeds(self):\n",
    "        \"\"\" freeze the positional embedding parameters of the model; adapted from finetune.py \"\"\"\n",
    "        self.freeze_params(self.model.model.shared)\n",
    "        for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "            self.freeze_params(d.embed_positions)\n",
    "            self.freeze_params(d.embed_tokens)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    # Do a forward pass through the model\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                **kwargs):\n",
    "        return self.model(input_ids.to(self.device),\n",
    "                          **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                     lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor,\n",
    "                           pad_token_id: torch.Tensor):\n",
    "        \"\"\" Shift input ids one token to the right,\n",
    "         and wrap the last non pad token (usually <eos>).\n",
    "                This is taken directly from modeling_bart.py\n",
    "        \"\"\"\n",
    "        prev_output_tokens = input_ids.clone()\n",
    "        index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "        prev_output_tokens[:, 0] = input_ids.gather(1,\n",
    "                                                    index_of_eos).squeeze()\n",
    "        prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
    "        return prev_output_tokens\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = 0\n",
    "        for data in batch:\n",
    "            input_ids_source, attention_mask_source = self.tokenizer(data[\"source\"],\n",
    "                                                                     truncation=True,\n",
    "                                                                     max_length=512,\n",
    "                                                                     padding=\"max_length\",\n",
    "                                                                     return_tensors='pt').values()\n",
    "            input_ids_target, attention_mask_target = self.tokenizer(data[\"target\"],\n",
    "                                                                     truncation=True,\n",
    "                                                                     max_length=512,\n",
    "                                                                     padding=\"max_length\",\n",
    "                                                                     return_tensors='pt').values()\n",
    "\n",
    "            # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "            decoder_input_ids = self.shift_tokens_right(input_ids_target,\n",
    "                                                        self.tokenizer.pad_token_id)\n",
    "\n",
    "            # Run the model and get the logits\n",
    "            outputs = self(input_ids_source.to(self.device),\n",
    "                           attention_mask=attention_mask_source.to(self.device),\n",
    "                           decoder_input_ids=decoder_input_ids.to(self.device),\n",
    "                           use_cache=False)\n",
    "            lm_logits = outputs[\"logits\"]\n",
    "            # Calculate the loss on the un-shifted tokens\n",
    "            loss += self.loss_fn(lm_logits.view(-1, lm_logits.shape[-1]).to(self.device),\n",
    "                                 input_ids_target.view(-1).to(self.device))\n",
    "            self.log('train/loss_step', loss.item(), on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "        self.log('train/loss_epoch', loss.item(), on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
    "        self.train_loss = loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        logging.info(f'Finishing  epoch {str(self.current_epoch).rjust(5)} - loss : {str(self.train_loss).rjust(15)}')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in batch:\n",
    "                input_ids_source, attention_mask_source = self.tokenizer(data[\"source\"],\n",
    "                                                                         truncation=True,\n",
    "                                                                         max_length=512,\n",
    "                                                                         padding=\"max_length\",\n",
    "                                                                         return_tensors='pt').values()\n",
    "                input_ids_target, attention_mask_target = self.tokenizer(data[\"target\"],\n",
    "                                                                         truncation=True,\n",
    "                                                                         max_length=512,\n",
    "                                                                         padding=\"max_length\",\n",
    "                                                                         return_tensors='pt').values()\n",
    "\n",
    "                # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "                decoder_input_ids = self.shift_tokens_right(input_ids_target,\n",
    "                                                            self.tokenizer.pad_token_id)\n",
    "\n",
    "                # Run the model and get the logits\n",
    "                outputs = self(input_ids_source.to(self.device),\n",
    "                               attention_mask=attention_mask_source.to(self.device),\n",
    "                               decoder_input_ids=decoder_input_ids.to(self.device),\n",
    "                               use_cache=False)\n",
    "                lm_logits = outputs[\"logits\"]\n",
    "                # Calculate the loss on the un-shifted tokens\n",
    "                loss += self.loss_fn(lm_logits.view(-1, lm_logits.shape[-1]).to(self.device),\n",
    "                                     input_ids_target.view(-1).to(self.device))\n",
    "                self.log('Val/loss_step', loss.item(), on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "            self.log('Val/loss_epoch', loss.item(), on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
    "            self.val_loss = loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        logging.info(f'Validation epoch {str(self.current_epoch).rjust(5)} - loss : {str(self.val_loss).rjust(15)}')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        pass\n",
    "\n",
    "    # # Method that generates text using the BartForConditionalGeneration's generate() method\n",
    "    # def generate_text(self,\n",
    "    #                   text,\n",
    "    #                   eval_beams,\n",
    "    #                   early_stopping=True,\n",
    "    #                   max_len=40):\n",
    "    #     \"\"\"Function to generate text\"\"\"\n",
    "    #\n",
    "    #     generated_ids = self.model.generate(\n",
    "    #         text[\"input_ids\"],\n",
    "    #         attention_mask=text[\"attention_mask\"],\n",
    "    #         use_cache=True,\n",
    "    #         decoder_start_token_id=self.tokenizer.pad_token_id,\n",
    "    #         num_beams=eval_beams,\n",
    "    #         max_length=max_len,\n",
    "    #         early_stopping=early_stopping\n",
    "    #     )\n",
    "    #     return [self.tokenizer.decode(w,\n",
    "    #                                   skip_special_tokens=True,\n",
    "    #                                   clean_up_tokenization_spaces=True) for w in generated_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load model and dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "text_column=\"w/o_heading_first_sentence\"\n",
    "save_path_checkpoints=\"./checkpoints\"\n",
    "save_path_model=\"./model\"\n",
    "model_name=\"BART_generator\"\n",
    "small=False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if small:\n",
    "    path_to_file_prefix = \"../../../data-subset_pre_processed/\"\n",
    "else:\n",
    "    path_to_file_prefix = \"/users/iris/rserrano/data-set_pre_processed/\"\n",
    "path_to_file_prefix = \"../../../data-set_pre_processed/\"\n",
    "\n",
    "ds_train, ds_val, ds_test = get_datasets(\n",
    "    path_to_file=path_to_file_prefix + \"fold-0/articles_train.json\",\n",
    "    path_to_corpus=path_to_file_prefix + \"fold-0/corpus_train.json\",\n",
    "    text_column=text_column,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model from a pre-saved checkpoint; alternatively use the code below to start training from scratch\n",
    "# model = LitModel.load_from_checkpoint(base_dir + \"checkpoint_files_2/8_ep_140k_simple_0210.ckpt\",\n",
    "#                                                                             learning_rate = 2e-5,\n",
    "#                                                                             tokenizer = tokenizer,\n",
    "#                                                                             model = bart_model,\n",
    "#                                                                             hparams = hparams)\n",
    "\n",
    "bart = Bart(train_val_test=(ds_train, ds_val, ds_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path_checkpoints = \"./checkpoints\"\n",
    "model_name = \"bart\"\n",
    "logger = pl_loggers.TensorBoardLogger(save_path_checkpoints, name=model_name)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"Val/loss_epoch\", mode=\"min\", save_top_k=2, every_n_epochs=1)\n",
    "\n",
    "trainer = Trainer(logger=logger,\n",
    "                  precision=32,\n",
    "                  accelerator=\"cpu\",\n",
    "                  # gpus=-1,\n",
    "                  # strategy='dp',\n",
    "                  max_epochs=100,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  log_every_n_steps=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model=bart)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}