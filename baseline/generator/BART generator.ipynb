{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import (BartTokenizer,\n",
    "                          BartForConditionalGeneration,\n",
    "                          AdamW,\n",
    "                          BartConfig)\n",
    "from torch.utils.data import (DataLoader,\n",
    "                              TensorDataset,\n",
    "                              random_split,\n",
    "                              Dataset)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "import operator\n",
    "import copy\n",
    "import logging\n",
    "import psutil\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from data_utils import train_val_test_split_df\n",
    "from file_utils import mkdir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# generated = model.generate(tokenizer(\"Hello world\",\n",
    "#                                      max_length=32,\n",
    "#                                      return_tensors=\"pt\",\n",
    "#                                      truncation=True)[\"input_ids\"],\n",
    "#                            attention_mask=tokenizer(\"Hello world\",\n",
    "#                                                     max_length=32,\n",
    "#                                                     return_tensors=\"pt\",\n",
    "#                                                     truncation=True)[\"attention_mask\"],\n",
    "#                            use_cache=True,\n",
    "#                            num_beams=4,\n",
    "#                            max_length=32,\n",
    "#                            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "#                            early_stopping=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CorpusDataset(pd.DataFrame):\n",
    "    def __init__(self, path_to_file: str):\n",
    "        super().__init__(self.get_df(path_to_file))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_df(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Create a dataloading module as per the PyTorch Lightning Docs\n",
    "class AnswerGenerationData(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_to_df: str = None,\n",
    "                 df: pd.DataFrame = None,\n",
    "                 text_column: str = None,\n",
    "                 corpus: CorpusDataset = None):\n",
    "\n",
    "        assert operator.xor(path_to_df is None, df is None)\n",
    "        self.df = self.get_dataset(path_to_df=path_to_df,\n",
    "                                   df=df,\n",
    "                                   text_column=text_column,\n",
    "                                   corpus=corpus)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dataset(path_to_df: str = None,\n",
    "                    df: pd.DataFrame = None,\n",
    "                    text_column: str = None,\n",
    "                    corpus: CorpusDataset = None):\n",
    "\n",
    "        def get_df(path=None, df=None, text_column=None):\n",
    "            def parse_ids(ids_str):\n",
    "                ids_list = [ids[1:-1] for ids in ids_str[1:-1].split(\", \")]\n",
    "                return ids_list\n",
    "\n",
    "            if path is not None:\n",
    "                df = pd.read_csv(path)\n",
    "\n",
    "            columns = [\"query\",\n",
    "                       \"outline\",\n",
    "                       \"text\",\n",
    "                       \"paragraphs_id\"]\n",
    "            df = df[[\"query\",\n",
    "                     \"outline\",\n",
    "                     \"text_\" + text_column,\n",
    "                     \"paragraphs_id\"]]\n",
    "            df = df.loc[df[\"outline\"].map(len) > 0]\n",
    "            df[\"text\"] = df[\"text_\" + text_column]\n",
    "            df[\"paragraphs_id\"] = df[\"paragraphs_id\"].apply(parse_ids)\n",
    "            df = df.loc[df[\"text\"] != \"\\n\"]\n",
    "            df = df[columns]\n",
    "            return df\n",
    "\n",
    "        def get_paragraphs(docs_id, corpus):\n",
    "            stack = \"\"\n",
    "            for index in docs_id:\n",
    "                stack += corpus[corpus[\"id\"] == index][\"text\"].item() + \"\\n\"\n",
    "            stack = stack[:-1]\n",
    "            return stack\n",
    "\n",
    "        assert operator.xor(path_to_df is not None, df is not None)\n",
    "\n",
    "        df = get_df(path_to_df, df, text_column)\n",
    "        corpus = corpus\n",
    "\n",
    "        df.rename(columns={\"text\": \"target\"}, inplace=True)\n",
    "        df[\"source\"] = df[\"paragraphs_id\"].apply(lambda ids: get_paragraphs(ids, corpus))\n",
    "        df = df[[\"source\", \"target\"]]  #, \"outline\"]]\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if type(item) == int:\n",
    "            return self.df[item:item + 1].to_dict('records')[0]\n",
    "        elif type(item) == str:\n",
    "            return self.df[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_datasets(path_to_file, path_to_corpus, text_column, ):\n",
    "    corpus = CorpusDataset(path_to_file=path_to_corpus)\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    df_train, df_val, df_test = train_val_test_split_df(df)\n",
    "    del df\n",
    "\n",
    "    df_train = AnswerGenerationData(df=df_train,\n",
    "                                    text_column=text_column,\n",
    "                                    corpus=corpus)\n",
    "    df_val = AnswerGenerationData(df=df_val,\n",
    "                                  text_column=text_column,\n",
    "                                  corpus=corpus)\n",
    "    df_test = AnswerGenerationData(df=df_test,\n",
    "                                   text_column=text_column,\n",
    "                                   corpus=corpus)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Bart(pl.LightningModule):\n",
    "    # Instantiate the model\n",
    "    def __init__(self,\n",
    "                 model_name='facebook/bart-base',\n",
    "                 train_val_test: tuple = None,\n",
    "                 freeze_encoder=True,\n",
    "                 freeze_embeds=True,\n",
    "                 eval_beams=4,\n",
    "                 batch_size=32,\n",
    "                 num_workers=20,\n",
    "                 learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eval_beams = eval_beams\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_ds, self.val_ds, self.test_ds = train_val_test\n",
    "\n",
    "        if freeze_encoder:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "\n",
    "        if freeze_embeds:\n",
    "            self.freeze_embeds()\n",
    "\n",
    "        if num_workers == -1:\n",
    "            logging.info(f\"Number of CPUs available : {psutil.cpu_count()}.\")\n",
    "            self.num_workers = psutil.cpu_count()\n",
    "        else:\n",
    "            self.num_workers = min(num_workers, psutil.cpu_count())\n",
    "\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def freeze_params(model):\n",
    "        \"\"\" Function that takes a model as input (or part of a model) and freezes the layers for faster training\n",
    "                adapted from finetune.py \"\"\"\n",
    "        for layer in model.parameters():\n",
    "            layer.requires_grade = False\n",
    "\n",
    "    def freeze_embeds(self):\n",
    "        \"\"\" freeze the positional embedding parameters of the model; adapted from finetune.py \"\"\"\n",
    "        self.freeze_params(self.model.model.shared)\n",
    "        for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "            self.freeze_params(d.embed_positions)\n",
    "            self.freeze_params(d.embed_tokens)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: x)\n",
    "\n",
    "    # Do a forward pass through the model\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                **kwargs):\n",
    "        return self.model(input_ids,\n",
    "                          **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                     lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor,\n",
    "                           pad_token_id: torch.Tensor):\n",
    "        \"\"\" Shift input ids one token to the right,\n",
    "         and wrap the last non pad token (usually <eos>).\n",
    "                This is taken directly from modeling_bart.py\n",
    "        \"\"\"\n",
    "        prev_output_tokens = input_ids.clone()\n",
    "        index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "        prev_output_tokens[:, 0] = input_ids.gather(1,\n",
    "                                                    index_of_eos).squeeze()\n",
    "        prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
    "        return prev_output_tokens\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = 0\n",
    "        for data in batch:\n",
    "            input_ids_source, attention_mask_source = self.tokenizer(data[\"source\"],\n",
    "                                                                     truncation=True,\n",
    "                                                                     max_length=512,\n",
    "                                                                     padding=\"max_length\",\n",
    "                                                                     return_tensors='pt').values()\n",
    "            input_ids_target, attention_mask_target = self.tokenizer(data[\"target\"],\n",
    "                                                                     truncation=True,\n",
    "                                                                     max_length=512,\n",
    "                                                                     padding=\"max_length\",\n",
    "                                                                     return_tensors='pt').values()\n",
    "\n",
    "            # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "            decoder_input_ids = self.shift_tokens_right(input_ids_target,\n",
    "                                                        self.tokenizer.pad_token_id)\n",
    "\n",
    "            # Run the model and get the logits\n",
    "            outputs = self(input_ids_source,\n",
    "                           attention_mask=attention_mask_source,\n",
    "                           decoder_input_ids=decoder_input_ids,\n",
    "                           use_cache=False)\n",
    "            lm_logits = outputs[\"logits\"]\n",
    "            # Calculate the loss on the un-shifted tokens\n",
    "            loss += self.loss_fn(lm_logits.view(-1, lm_logits.shape[-1]),\n",
    "                                 input_ids_target.view(-1))\n",
    "            self.log('train/loss_step', loss.item(), on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "        self.log('train/loss_epoch', loss.item(), on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        logging.info(f'Finishing  epoch {str(self.current_epoch).rjust(5)} - loss : {str(self.train_loss).rjust(15)}')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in batch:\n",
    "                input_ids_source, attention_mask_source = self.tokenizer(data[\"source\"],\n",
    "                                                                         truncation=True,\n",
    "                                                                         max_length=512,\n",
    "                                                                         padding=\"max_length\",\n",
    "                                                                         return_tensors='pt').values()\n",
    "                input_ids_target, attention_mask_target = self.tokenizer(data[\"target\"],\n",
    "                                                                         truncation=True,\n",
    "                                                                         max_length=512,\n",
    "                                                                         padding=\"max_length\",\n",
    "                                                                         return_tensors='pt').values()\n",
    "\n",
    "                # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "                decoder_input_ids = self.shift_tokens_right(input_ids_target,\n",
    "                                                            self.tokenizer.pad_token_id)\n",
    "\n",
    "                # Run the model and get the logits\n",
    "                outputs = self(input_ids_source,\n",
    "                               attention_mask=attention_mask_source,\n",
    "                               decoder_input_ids=decoder_input_ids,\n",
    "                               use_cache=False)\n",
    "                lm_logits = outputs[\"logits\"]\n",
    "                # Calculate the loss on the un-shifted tokens\n",
    "                loss += self.loss_fn(lm_logits.view(-1, lm_logits.shape[-1]),\n",
    "                                     input_ids_target.view(-1))\n",
    "                self.log('Val/loss_step', loss.item(), on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "            self.log('Val/loss_epoch', loss.item(), on_step=False, on_epoch=True, batch_size=self.batch_size)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        logging.info(f'Validation epoch {str(self.current_epoch).rjust(5)} - loss : {str(self.val_loss).rjust(15)}')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        pass\n",
    "\n",
    "    # # Method that generates text using the BartForConditionalGeneration's generate() method\n",
    "    # def generate_text(self,\n",
    "    #                   text,\n",
    "    #                   eval_beams,\n",
    "    #                   early_stopping=True,\n",
    "    #                   max_len=40):\n",
    "    #     \"\"\"Function to generate text\"\"\"\n",
    "    #\n",
    "    #     generated_ids = self.model.generate(\n",
    "    #         text[\"input_ids\"],\n",
    "    #         attention_mask=text[\"attention_mask\"],\n",
    "    #         use_cache=True,\n",
    "    #         decoder_start_token_id=self.tokenizer.pad_token_id,\n",
    "    #         num_beams=eval_beams,\n",
    "    #         max_length=max_len,\n",
    "    #         early_stopping=early_stopping\n",
    "    #     )\n",
    "    #     return [self.tokenizer.decode(w,\n",
    "    #                                   skip_special_tokens=True,\n",
    "    #                                   clean_up_tokenization_spaces=True) for w in generated_ids]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load model and dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_file_prefix = \"../../../data-subset_pre_processed/\"\n",
    "text_column = \"w/o_heading_first_sentence_by_paragraph\"\n",
    "nb_irrelevant = 2\n",
    "\n",
    "ds_train, ds_val, ds_test = get_datasets(\n",
    "    path_to_file=path_to_file_prefix + \"fold-1/articles_train_all_ids.csv\",\n",
    "    path_to_corpus=path_to_file_prefix + \"fold-1/corpus_train.csv\",\n",
    "    text_column=text_column,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model from a pre-saved checkpoint; alternatively use the code below to start training from scratch\n",
    "# model = LitModel.load_from_checkpoint(base_dir + \"checkpoint_files_2/8_ep_140k_simple_0210.ckpt\",\n",
    "#                                                                             learning_rate = 2e-5,\n",
    "#                                                                             tokenizer = tokenizer,\n",
    "#                                                                             model = bart_model,\n",
    "#                                                                             hparams = hparams)\n",
    "\n",
    "bart = Bart(train_val_test=(ds_train, ds_val, ds_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path_checkpoints = \"./checkpoints\"\n",
    "model_name = \"bart\"\n",
    "logger = pl_loggers.TensorBoardLogger(save_path_checkpoints, name=model_name)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"Val/loss_epoch\", mode=\"min\", save_top_k=2, every_n_epochs=1)\n",
    "\n",
    "trainer = Trainer(logger=logger,\n",
    "                  precision=32,\n",
    "                  accelerator=\"cpu\",\n",
    "                  # gpus=-1,\n",
    "                  # strategy='dp',\n",
    "                  max_epochs=100,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "                  log_every_n_steps=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model=bart)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for batch in bart.train_dataloader():\n",
    "    for data in batch:\n",
    "        input_ids_source, attention_mask_source = bart.tokenizer(data[\"source\"],\n",
    "                                                                 truncation=True,\n",
    "                                                                 max_length=512,\n",
    "                                                                 padding=\"max_length\",\n",
    "                                                                 return_tensors='pt').values()\n",
    "        input_ids_target, attention_mask_target = bart.tokenizer(data[\"target\"],\n",
    "                                                                 truncation=True,\n",
    "                                                                 max_length=512,\n",
    "                                                                 padding=\"max_length\",\n",
    "                                                                 return_tensors='pt').values()\n",
    "\n",
    "        # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "        decoder_input_ids = bart.shift_tokens_right(input_ids_target,\n",
    "                                                    bart.tokenizer.pad_token_id)\n",
    "\n",
    "        # Run the model and get the logits\n",
    "        outputs = bart(input_ids_source,\n",
    "                       attention_mask=attention_mask_source,\n",
    "                       decoder_input_ids=decoder_input_ids,\n",
    "                       use_cache=False)\n",
    "        break\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}